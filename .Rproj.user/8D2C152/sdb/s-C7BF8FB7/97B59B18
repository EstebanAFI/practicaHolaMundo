{
    "contents" : "library(dplyr)\nauto <- read.csv('auto.csv', sep = ';', header = T, fileEncoding = 'utf-8')\n#mpg frente a horsepower\nattach(auto)\nplot(mpg,horsepower)\nlm.mpg.horsepower <-  lm(mpg ~ horsepower, data = auto)\n# alt gr 4 mas espacio\nlm.mpg.horsepower\nsummary(lm.mpg.horsepower)\nconfint(lm.mpg.horsepower, level = 0.95)\n#representar datos vs modelo\nplot(horsepower,mpg, type = 'p', col = 'red', ylab = 'mpg', xlab = 'horsepower')\nabline(lm.mpg.horsepower, col = 'blue')\n\nplot(horsepower, lm.mpg.horsepower$residuals, type = 'p', col = 'red', xlab = 'TV', ylab = 'erro')\n\n# se tiene que llamar igual que la variable\nnuevo_dato = data.frame(horsepower = c(98))\npredicted_values <- predict(lm.mpg.horsepower, nuevo_dato, interval = 'confidence')\n\n\nlm_1<-lm(log(mpg)~log(horsepower),data=auto)\n\npar(mfrow=c(1,2))\n\n# la maldicion de la multidimension\n#introduction statical learning\n\npairs(auto,colors=red)\n\nlm.multi <- lm(mpg~displacement + horsepower + weight,data=auto)\nsummary(lm.multi)\n\n\nlm.multi <- lm(mpg~horsepower + weight,data=auto)\nsummary(lm.multi)\n\nlm.multi <- lm(mpg~ year + weight,data=auto)\nsummary(lm.multi) # 0.8072\n\n\npar(mfrow = c(1,2))\n\nplot(mpg, lm.multi$fitted.values, type = 'p', col = 'red', xlab = 'mpg', ylab = 'Predicted mpg')\nplot(mpg, lm.multi$residuals, type = 'p', col = 'red', xlab = 'mpg', ylab = 'Residuals')\n\nlm.multi <- lm(log(mpg)~ year + weight,data=auto)\nsummary(lm.multi) # 0.87\n\nlm.multi <- lm(log(mpg)~ (year) + log(weight) + (acceleration) + log(displacement),data=auto)\nsummary(lm.multi) # 0.8813\n\n\n\n\nlm.multi <- lm(mpg~ year + weight + horsepower,data=auto)\nsummary(lm.multi) # 0.068\n\ncor(auto[,1:8])\n\n#########  SEGUNDA CLASE  #############################################\n# analisis exploratorio\n# ajuste un primer modelo de regresion linial\n# seleccion de variables (backward, forward)\n# identificacion de relaciones no lineales y transformacion de predictores\n# tienen varianza constate los residuos.Trasnformacon de variales respuestas\n# identficacion de outliers y(o puntos high leverage) ¿se quitan o no?\n# existe multicolinealidad\n# vuelta a tres\n\ninstall.packages('locfit')\ninstall.packages('leaps')\nlibrary(locfit)\nlibrary(leaps)\n\nfull_model <-\n  lm(mpg ~ year + weight + horsepower + cylinders + displacement + acceleration + origin , data = auto)\n#.- name\nprint(full_model)\n\n\nAIC(full_model)\nBIC(full_model)\ncp(full_model)\nsummary(full_model)$adj.r.squared\n\n\n\n\n",
    "created" : 1478194534098.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1779878623",
    "id" : "97B59B18",
    "lastKnownWriteTime" : 1477557537,
    "path" : "C:/Users/jfuente/Desktop/Regresión lineal - Daniel Girela/1º clase/ejemplos de regresiones alberto.R",
    "project_path" : null,
    "properties" : {
    },
    "relative_order" : 4,
    "source_on_save" : false,
    "type" : "r_source"
}